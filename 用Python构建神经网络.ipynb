{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Python构建神经网络\n",
    "\n",
    "本节将使用Python语言和Numpy库来构建神经网络模型，向读者展示神经网络的基本概念和工作过程。\n",
    "\n",
    "\n",
    "## 构建神经网络/深度学习模型的基本步骤\n",
    "\n",
    "如之前的介绍，应用于不同场景的深度学习模型具备一定的通用性，均可以从下述五个步骤来完成模型的构建和训练。\n",
    "- 数据处理：从本地文件或网络地址读取数据，并做预处理操作，如校验数据的正确性等。\n",
    "- 模型设计：完成网络结构的设计（模型要素1），相当于模型的假设空间，即模型能够表达的关系集合。\n",
    "- 训练配置：设定模型采用的寻解算法（模型要素2），即优化器，并指定计算资源。\n",
    "- 训练过程：循环调用训练过程，每轮均包括前向计算 、损失函数（优化目标，模型要素3）和后向传播这三个步骤。\n",
    "- 保存模型：将训练好的模型保存，以备预测时调用。\n",
    "\n",
    "下面使用Python编写预测波士顿房价的模型，一样遵循这样的五个步骤。\n",
    "正是由于这个建模和训练的过程存在通用性，即不同的模型仅仅在模型三要素上不同，而五个步骤中的其它部分保持一致，深度学习框架才有用武之地。\n",
    "\n",
    "\n",
    "## 波士顿房价预测\n",
    "\n",
    "波士顿房价预测是一个经典的机器学习问题，类似于程序员世界的“Hello World”。波士顿地区的房价是由诸多因素影响的，该数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素预测房价的模型。预测问题根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。下面我们尝试用最简单的线性回归模型解决这个问题，并用神经网络来实现这个模型。\n",
    "\n",
    "## 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述（类似牛顿第二定律的案例）：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$和$b$就是直线的斜率和截距。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "在搭建模型之前，让我们先导入数据，查阅下内容。房价数据存放在本地目录下的housing.data文件中，通过执行如下的代码可以导入数据并查阅。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.397829Z",
     "start_time": "2021-04-27T13:02:29.373894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,\n",
       "       1.190e+01])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import json\n",
    "# 读入训练数据\n",
    "datafile = 'housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为读入的原始数据是1维的，所有数据都连在了一起。所以将数据的形状进行变换，形成一个2维的矩阵。每行为一个数据样本（14个值），每个数据样本包含13个X（影响房价的特征）和一个Y（该类型房屋的均价）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.411053Z",
     "start_time": "2021-04-27T13:02:29.399826Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读入之后的数据被转化成1维array，其中array的\n",
    "# 第0-13项是第一条数据，第14-27项是第二条数据，.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names)\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.425724Z",
     "start_time": "2021-04-27T13:02:29.412052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "x = data[0]\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取80%的数据作为训练集，预留20%的数据用于测试模型的预测效果（训练好的模型预测值与实际房价的差距）。打印训练集的形状可见，我们共有404个样本，每个样本含有13个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.441289Z",
     "start_time": "2021-04-27T13:02:29.427746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "offset = int(data.shape[0] * ratio)\n",
    "training_data = data[:offset]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：\n",
    "1. 模型训练更高效。\n",
    "2. 特征前的权重大小可代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.456859Z",
     "start_time": "2021-04-27T13:02:29.443286Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算train数据集的最大值，最小值，平均值\n",
    "maximums, minimums, avgs = \\\n",
    "                     training_data.max(axis=0), \\\n",
    "                     training_data.min(axis=0), \\\n",
    "     training_data.sum(axis=0) / training_data.shape[0]\n",
    "# 对数据进行归一化处理\n",
    "for i in range(feature_num):\n",
    "    #print(maximums[i], minimums[i], avgs[i])\n",
    "    data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述几个数据处理操作合并成load data函数，并确认函数的执行效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.472617Z",
     "start_time": "2021-04-27T13:02:29.457896Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = 'housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算train数据集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.485144Z",
     "start_time": "2021-04-27T13:02:29.473616Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "training_data, test_data = load_data()\n",
    "x = training_data[:, :-1]\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.497844Z",
     "start_time": "2021-04-27T13:02:29.487176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817\n",
      "  0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112\n",
      " -0.17590923]\n",
      "[-0.00390539]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果将输入特征和输出预测值均以向量表示，输入特征x一共有13个分量，y只有1个分量，所以参数权重的形状（shape）应该是$13\\times1$。假设我们以如下任意数字赋值参数做初始化：\n",
    "$w=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3，-0.4, 0.0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.514470Z",
     "start_time": "2021-04-27T13:02:29.498842Z"
    }
   },
   "outputs": [],
   "source": [
    "w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]\n",
    "w = np.array(w).reshape([13, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出第1条样本数据，观察样本的特征向量与参数向量相乘之后的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.529655Z",
     "start_time": "2021-04-27T13:02:29.516496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03395597]\n"
     ]
    }
   ],
   "source": [
    "x1=x[0]\n",
    "t = np.dot(x1, w)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，完整的线性回归公式，还需要初始化偏移量$b$，同样随意赋初值-0.2。\n",
    "那么，线性回归模型的完整输出是$z=t+b$，这个从特征和参数计算输出值的过程称为“前向计算”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.543818Z",
     "start_time": "2021-04-27T13:02:29.530878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16604403]\n"
     ]
    }
   ],
   "source": [
    "b = -0.2\n",
    "z = t + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络\n",
    "\n",
    "将上述计算预测输出的过程以“类和对象”的方式来描述，实现的方案如下所示。类成员变量有参数 w 和 b，并写了一个forward函数（代表“前向计算”）完成上述从特征和参数到输出预测值的计算过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.557659Z",
     "start_time": "2021-04-27T13:02:29.544970Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，\n",
    "        # 此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Network类的定义，模型的计算过程可以按下述方式达成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.576758Z",
     "start_time": "2021-04-27T13:02:29.559656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63182506]\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z = net.forward(x1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过模型计算$x_1$表示的影响因素所对应的房价应该是$z$, 但实际数据告诉我们房价是$y$，这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，具体定义如下：\n",
    "$$Loss = (y - z)^2$$\n",
    "上式中的$Loss$(简记为: $L$) 通常也被称作损失函数，它是衡量模型好坏的指标，在回归问题中均方误差是一种比较常见的形式，分类问题中通常会采用交叉熵损失函数，在后续的章节中会更详细的介绍。\n",
    "对一个样本计算损失的代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.591708Z",
     "start_time": "2021-04-27T13:02:29.578582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39428312]\n"
     ]
    }
   ],
   "source": [
    "Loss = (y1 - z)*(y1 - z)\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为计算损失时需要把每个样本的损失都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。\n",
    "$$L= \\frac{1}{N}\\sum_i{(y^{(i)} - z^{(i)})^2}$$\n",
    "对上面的计算代码做出相应的调整，在Network类下面添加损失函数的计算过程如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.604054Z",
     "start_time": "2021-04-27T13:02:29.593704Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        cost = error * error\n",
    "        cost = np.mean(cost)\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面定义的Network类，可以方便的计算预测值和损失函数。\n",
    "需要注意，类中的变量x, w，b, z, error等均是向量。以变量x为例，共有两个维度，一个代表特征数量（=13），一个代表样本数量（演示程序如下）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:29.619286Z",
     "start_time": "2021-04-27T13:02:29.605756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  [[-0.63182506]\n",
      " [-0.55793096]\n",
      " [-1.00062009]]\n",
      "loss: 0.7229825055441158\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "# 此处可以一次性计算多个样本的预测值和损失函数\n",
    "x1 = x[0:3]\n",
    "y1 = y[0:3]\n",
    "z = net.forward(x1)\n",
    "print('predict: ', z)\n",
    "loss = net.loss(z, y1)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的训练\n",
    "\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来将介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练。模型训练的目标是让定义的损失函数尽可能的小，也就是说找到一个参数解$w$和$b$使得损失函数取得极小值。\n",
    "\n",
    "### 求解损失函数的极小值\n",
    "\n",
    "基于最基本的微积分知识，函数在极值点处的导数为0。那么，让损失函数取极小值的$w$和$b$应该是下述方程组的解：\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}}=0,  \\ \\ for \\ \\ \\ j = 0, ..., 12$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0$$\n",
    "\n",
    "将样本数据$(x, y)$带入上面的方程组固然可以求解出$w$和$b$的值，但是这种方法只对线性回归这样简单的情况有效。如果模型中含有非线性变换，或者损失函数不是均方差这种简单形式，则很难通过上式求解。为了避免这一情况，下面我们将引入更加普适的数值求解方法。\n",
    "\n",
    "### 梯度下降法\n",
    "\n",
    "训练的关键是找到一组$(w, b)$使得损失函数$L$取极小值。我们先看一下损失函数$L$只随两个参数变化时的简单情形，启发下寻解的思路。\n",
    "$$L=L(w_5, w_9)$$\n",
    "这里我们将$w_0, w_1, ..., w_{12}$中除$w_5, w_9$之外的参数和$b$都固定下来，可以用图画出$L(w_5, w_9)$的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.464297Z",
     "start_time": "2021-04-27T13:02:29.621257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = Network(13)\n",
    "losses = []\n",
    "#只画出参数w5和w9在区间[-160, 160]的曲线部分，已经包含损失函数的极值\n",
    "w5 = np.arange(-160.0, 160.0, 1.0)\n",
    "w9 = np.arange(-160.0, 160.0, 1.0)\n",
    "losses = np.zeros([len(w5), len(w9)])\n",
    "\n",
    "#计算设定区域内每个参数取值所对应的Loss\n",
    "for i in range(len(w5)):\n",
    "    for j in range(len(w9)):\n",
    "        net.w[5] = w5[i]\n",
    "        net.w[9] = w9[j]\n",
    "        z = net.forward(x)\n",
    "        loss = net.loss(z, y)\n",
    "        losses[i, j] = loss\n",
    "\n",
    "#将两个变量和对应的Loss作3D图\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "w5, w9 = np.meshgrid(w5, w9)\n",
    "\n",
    "ax.plot_surface(w5, w9, losses, rstride=1, cstride=1, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单情形——只考虑两个参数$w_5$和$w_9$\n",
    "\n",
    "对于这种简单情形，我们利用上面的程序在3维空间中画出了损失函数随参数变化的曲面图，从上图可以看出有些区域的函数值明显比周围的点小。需要说明的是：为什么这里我们选择$w_5$和$w_9$来画图？这是因为选择这两个参数的时候，可比较直观的从损失函数的曲面图上发现极值点的存在。其他参数组合，从图形上观测损失函数的极值点不够直观。\n",
    "\n",
    "现在我们要找出一组$[w_5, w_9]$的值，使得损失函数最小，实现梯度下降法的方案如下：\n",
    "\n",
    "- 随机的选一组初始值，例如：\n",
    "    $[w_5, w_9] = [-100.0, -100.0]$\n",
    "- 选取下一个点$[w_5^{'} , w_9^{'}]$使得\n",
    "   $L(w_5^{'} , w_9^{'}) < L(w_5, w_9)$\n",
    "- 重复上面的步骤2，直到损失函数几乎不再下降\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b8e37d9e937d41c89c34af3884ad05947d5a9ec02316467da1b6012f9b0635c4)\n",
    "\n",
    "图1-2-1 ：梯度下降方向示意图\n",
    "\n",
    "如何选择$[w_5^{'} , w_9^{'}]$是至关重要的，第一要保证$L$是下降的，第二要使得下降的趋势尽可能的快。微积分的基础知识告诉我们，沿着梯度的反方向，是函数值下降最快的方向，如下图所示在点$P_0$，$[w_5, w_9] = [-100.0, -100.0]$，梯度方向是图中$P_0$点的箭头指向的方向，沿着箭头方向向前移动一小步，可以观察损失函数的变化。\n",
    "在$P_0$点，$[w_5, w_9] = [-150.0, -150.0]$，可以计算出，此时的loss在1300左右。\n",
    "\n",
    "### 计算梯度\n",
    "\n",
    "上面我们讲过了损失函数的计算方法，这里稍微加以改写，引入因子$\\frac{1}{2}$，定义损失函数如下\n",
    "$$L= \\frac{1}{2N}\\sum_{i=1}^N{(y^{(i)} - z^{(i)})^2}$$\n",
    "其中$z_i$是网络对第$i$个样本的预测值\n",
    "$$z^{(i)} = \\sum_{j=0}^{12}{x_j^{(i)} w^{(j)}} + b$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})\\frac{\\partial{z^{(i)}}}{w_j}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})x_j^{(i)}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})\\frac{\\partial{z^{(i)}}}{b}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})}$$\n",
    "\n",
    "从导数的计算过程可以看出，因子$\\frac{1}{2}$被消掉了，这是因为二次函数求导的时候会产生因子$2$，这也是我们将损失函数改写的原因\n",
    "\n",
    "这里我们感兴趣的是$w_5$和$w_9$，\n",
    "$$\\frac{\\partial{C}}{\\partial{w_5}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})x_5^{(i)}}$$\n",
    "$$\\frac{\\partial{C}}{\\partial{w_9}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})x_9^{(i)}}$$\n",
    "\n",
    "则可以在Network类中定义如下的梯度计算函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度计算公式\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})\\frac{\\partial{z_j^{(i)}}}{w_j}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})x_j^{(i)}}$$\n",
    "\n",
    "借助于numpy里面的矩阵操作，我们可以直接对所有$w_j \\  (j = 0, ..., 12)$ 一次性的计算出13个参数所对应的梯度来\n",
    "\n",
    "先考虑只有一个样本的情况，上式中的$N=1$，$\\frac{\\partial{L}}{\\partial{w_j}}=(z^{(1)} - y^{(1)})x_j^{(1)}$\n",
    "\n",
    "可以通过具体的程序查看每个变量的数据和维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.477504Z",
     "start_time": "2021-04-27T13:02:36.465349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 [-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817\n",
      "  0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112\n",
      " -0.17590923], shape (13,)\n",
      "y1 [-0.00390539], shape (1,)\n",
      "z1 [-12.05947643], shape (1,)\n"
     ]
    }
   ],
   "source": [
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z1 = net.forward(x1)\n",
    "print('x1 {}, shape {}'.format(x1, x1.shape))\n",
    "print('y1 {}, shape {}'.format(y1, y1.shape))\n",
    "print('z1 {}, shape {}'.format(z1, z1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按上面的公式，当只有一个样本时，可以计算某个$w_j$，比如$w_0$的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.493219Z",
     "start_time": "2021-04-27T13:02:36.479499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w0 [0.25875126]\n"
     ]
    }
   ],
   "source": [
    "gradient_w0 = (z1 - y1) * x1[0]\n",
    "print('gradient_w0 {}'.format(gradient_w0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样我们可以计算$w_1$的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.510036Z",
     "start_time": "2021-04-27T13:02:36.495450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w1 [-0.45417275]\n"
     ]
    }
   ],
   "source": [
    "gradient_w1 = (z1 - y1) * x1[1]\n",
    "print('gradient_w1 {}'.format(gradient_w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依次计算$w_2$的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.526918Z",
     "start_time": "2021-04-27T13:02:36.511470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w1 [3.44214394]\n"
     ]
    }
   ],
   "source": [
    "gradient_w2= (z1 - y1) * x1[2]\n",
    "print('gradient_w1 {}'.format(gradient_w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一个for循环即可计算从$w_0$到$w_{12}$的所有权重的梯度，但Numpy给我们提供了更简单的操作方法，即使用矩阵操作。\n",
    "计算梯度的代码中直接用  (z1 - y1) * x1，得到的是一个13维的向量，每个分量分别代表该维度的梯度。\n",
    "Numpy的广播功能（对向量和矩阵计算如同对1个单一变量计算一样）是我们使用它的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.542061Z",
     "start_time": "2021-04-27T13:02:36.528915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample1 [ 0.25875126 -0.45417275  3.44214394  1.04441828 -0.15548386 -0.55875363\n",
      " -0.09591377  0.09232085  3.03465138  1.43234507  3.49642036 -0.62581917\n",
      "  2.12068622], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = (z1 - y1) * x1\n",
    "print('gradient_w_by_sample1 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再回到上面的梯度计算公式\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})\\frac{\\partial{z^{(i)}}}{w_j}} = \\frac{1}{N}\\sum_i^N{(z^{(i)} - y^{(i)})x_j^{(i)}}$$\n",
    "\n",
    "这里输入数据中有多个样本，每个样本都对梯度有贡献。如上代码计算了只有样本1时的梯度值，同样的计算方法也可以计算样本2和样本3对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.557447Z",
     "start_time": "2021-04-27T13:02:36.544058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample2 [ 0.7329239   4.91417754  3.33394253  2.9912385   4.45673435 -0.58146277\n",
      " -5.14623287 -2.4894594   7.19011988  7.99471607  0.83100061 -1.79236081\n",
      "  2.11028056], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "x2 = x[1]\n",
    "y2 = y[1]\n",
    "z2 = net.forward(x2)\n",
    "gradient_w = (z2 - y2) * x2\n",
    "print('gradient_w_by_sample2 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.570498Z",
     "start_time": "2021-04-27T13:02:36.559211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample3 [ 0.25138584  1.68549775  1.14349809  1.02595515  1.5286008  -1.93302947\n",
      "  0.4058236  -0.85385157  2.46611579  2.74208162  0.28502219 -0.46695229\n",
      "  2.39363651], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "x3 = x[2]\n",
    "y3 = y[2]\n",
    "z3 = net.forward(x3)\n",
    "gradient_w = (z3 - y3) * x3\n",
    "print('gradient_w_by_sample3 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用for循环把每个样本对梯度的贡献都计算出来，然后再作平均。\n",
    "\n",
    "但是我们不需要这么做，仍然可以使用Numpy的矩阵操作来简化运算，比如三个样本的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.586124Z",
     "start_time": "2021-04-27T13:02:36.572489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817\n",
      "   0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112\n",
      "  -0.17590923]\n",
      " [-0.02122729 -0.14232673 -0.09655922 -0.08663366 -0.12907805  0.0168406\n",
      "   0.14904763  0.0721009  -0.20824365 -0.23154675 -0.02406783  0.0519112\n",
      "  -0.06111894]\n",
      " [-0.02122751 -0.14232673 -0.09655922 -0.08663366 -0.12907805  0.1632288\n",
      "  -0.03426854  0.0721009  -0.20824365 -0.23154675 -0.02406783  0.03943037\n",
      "  -0.20212336]], shape (3, 13)\n",
      "y [[-0.00390539]\n",
      " [-0.05723872]\n",
      " [ 0.23387239]], shape (3, 1)\n",
      "z [[-12.05947643]\n",
      " [-34.58467747]\n",
      " [-11.60858134]], shape (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# 注意这里是一次取出3个样本的数据，不是取出第3个样本\n",
    "x3samples = x[0:3]\n",
    "y3samples = y[0:3]\n",
    "z3samples = net.forward(x3samples)\n",
    "\n",
    "print('x {}, shape {}'.format(x3samples, x3samples.shape))\n",
    "print('y {}, shape {}'.format(y3samples, y3samples.shape))\n",
    "print('z {}, shape {}'.format(z3samples, z3samples.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的x3samples, y3samples, z3samples的第一维大小均为3，表示有3个样本。下面计算这3个样本对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.601553Z",
     "start_time": "2021-04-27T13:02:36.588131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w [[ 0.25875126 -0.45417275  3.44214394  1.04441828 -0.15548386 -0.55875363\n",
      "  -0.09591377  0.09232085  3.03465138  1.43234507  3.49642036 -0.62581917\n",
      "   2.12068622]\n",
      " [ 0.7329239   4.91417754  3.33394253  2.9912385   4.45673435 -0.58146277\n",
      "  -5.14623287 -2.4894594   7.19011988  7.99471607  0.83100061 -1.79236081\n",
      "   2.11028056]\n",
      " [ 0.25138584  1.68549775  1.14349809  1.02595515  1.5286008  -1.93302947\n",
      "   0.4058236  -0.85385157  2.46611579  2.74208162  0.28502219 -0.46695229\n",
      "   2.39363651]], gradient.shape (3, 13)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = (z3samples - y3samples) * x3samples\n",
    "print('gradient_w {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处可见，计算梯度gradient_w的维度是$3 \\times 13$，并且其第1行与上面第1个样本计算的梯度gradient_w_by_sample1一致，第2行与上面第2个样本计算的梯度gradient_w_by_sample1一致，第3行与上面第3个样本计算的梯度gradient_w_by_sample1一致。这里使用矩阵操作，可能更加方便的对3个样本分别计算各自对梯度的贡献。\n",
    "\n",
    "那么对于有N个样本的情形，我们可以直接使用如下方式计算出所有样本对梯度的贡献，这就是使用Numpy库广播功能带来的便捷。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.620011Z",
     "start_time": "2021-04-27T13:02:36.603150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w shape (404, 13)\n",
      "[[  0.25875126  -0.45417275   3.44214394 ...   3.49642036  -0.62581917\n",
      "    2.12068622]\n",
      " [  0.7329239    4.91417754   3.33394253 ...   0.83100061  -1.79236081\n",
      "    2.11028056]\n",
      " [  0.25138584   1.68549775   1.14349809 ...   0.28502219  -0.46695229\n",
      "    2.39363651]\n",
      " ...\n",
      " [ 14.70025543 -15.10890735  36.23258734 ...  24.54882966   5.51071122\n",
      "   26.26098922]\n",
      " [  9.29832217 -15.33146159  36.76629344 ...  24.91043398  -1.27564923\n",
      "   26.61808955]\n",
      " [ 19.55115919 -10.8177237   25.94192351 ...  17.5765494    3.94557661\n",
      "   17.64891012]]\n"
     ]
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "print('gradient_w shape {}'.format(gradient_w.shape))\n",
    "print(gradient_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面gradient_w的每一行代表了一个样本对梯度的贡献。根据梯度的计算公式，总梯度是对每个样本对梯度贡献的平均值。\n",
    "我们也可以使用Numpy的均值函数来完成此过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.635878Z",
     "start_time": "2021-04-27T13:02:36.622005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w  (13,)\n",
      "w  (13, 1)\n",
      "[ 1.59697064 -0.92928123  4.72726926  1.65712204  4.96176389  1.18068454\n",
      "  4.55846519 -3.37770889  9.57465893 10.29870662  1.3900257  -0.30152215\n",
      "  1.09276043]\n",
      "[[ 1.76405235e+00]\n",
      " [ 4.00157208e-01]\n",
      " [ 9.78737984e-01]\n",
      " [ 2.24089320e+00]\n",
      " [ 1.86755799e+00]\n",
      " [ 1.59000000e+02]\n",
      " [ 9.50088418e-01]\n",
      " [-1.51357208e-01]\n",
      " [-1.03218852e-01]\n",
      " [ 1.59000000e+02]\n",
      " [ 1.44043571e-01]\n",
      " [ 1.45427351e+00]\n",
      " [ 7.61037725e-01]]\n"
     ]
    }
   ],
   "source": [
    "# axis = 0 表示把每一行做相加然后再除以总的行数\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "print('gradient_w ', gradient_w.shape)\n",
    "print('w ', net.w.shape)\n",
    "print(gradient_w)\n",
    "print(net.w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用numpy的矩阵操作方便的完成了gradient的计算，但引入了一个问题，gradient_w的形状是(13,)，而w的维度是(13, 1)。导致该问题的原因是使用np.mean函数的时候消除了第0维。为了加减乘除等计算方便，gradient_w和w必须保持一致的形状。所以，我们将gradient_w的维度也设置为(13, 1)，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.651962Z",
     "start_time": "2021-04-27T13:02:36.637897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w shape (13, 1)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "print('gradient_w shape', gradient_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合上面的讨论，我们可以把计算梯度的代码整理如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.667142Z",
     "start_time": "2021-04-27T13:02:36.653988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.59697064],\n",
       "       [-0.92928123],\n",
       "       [ 4.72726926],\n",
       "       [ 1.65712204],\n",
       "       [ 4.96176389],\n",
       "       [ 1.18068454],\n",
       "       [ 4.55846519],\n",
       "       [-3.37770889],\n",
       "       [ 9.57465893],\n",
       "       [10.29870662],\n",
       "       [ 1.3900257 ],\n",
       "       [-0.30152215],\n",
       "       [ 1.09276043]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "gradient_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码非常简洁的完成了$w$的梯度计算。同样，计算$b$的梯度的代码也是类似的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.681619Z",
     "start_time": "2021-04-27T13:02:36.669163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0918438870293816e-13"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_b = (z - y)\n",
    "gradient_b = np.mean(gradient_b)\n",
    "# 此处b是一个数值，所以可以直接用np.mean得到一个标量\n",
    "gradient_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面计算$w$和$b$的梯度的过程，写成Network类的gradient函数，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.694086Z",
     "start_time": "2021-04-27T13:02:36.682918Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)\n",
    "        \n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.709608Z",
     "start_time": "2021-04-27T13:02:36.695324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-100.0, -100.0], loss 686.3005008179159\n",
      "gradient [-0.850073323995813, -6.138412364807849]\n"
     ]
    }
   ],
   "source": [
    "# 调用上面定义的gradient函数，计算梯度\n",
    "# 初始化网络，\n",
    "net = Network(13)\n",
    "# 设置[w5, w9] = [-100., +100.]\n",
    "net.w[5] = -100.0\n",
    "net.w[9] = -100.0\n",
    "\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找损失函数更小的点\n",
    "\n",
    "下面我们开始研究怎样更新梯度，首先沿着梯度的反方向移动一小步下下一个点P1，观察损失函数的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:36.723058Z",
     "start_time": "2021-04-27T13:02:36.712729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-99.91499266760042, -99.38615876351922], loss 678.6472185028845\n",
      "gradient [-0.8556356178645292, -6.0932268634065805]\n"
     ]
    }
   ],
   "source": [
    "# 在[w5, w9]平面上，沿着梯度的反方向移动到下一个点P1\n",
    "# 定义移动步长 eta\n",
    "eta = 0.1\n",
    "# 更新参数w5和w9\n",
    "net.w[5] = net.w[5] - eta * gradient_w5\n",
    "net.w[9] = net.w[9] - eta * gradient_w9\n",
    "# 重新计算z和loss\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面的代码，可以发现沿着梯度反方向走一小步，下一个点的损失函数的确减少了。\n",
    "\n",
    "- 读者可以不停的点击上面的代码块，观察损失函数是否一直在变小。\n",
    "\n",
    "将上面的循环的计算过程封装在train和update函数中，如下代码所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.121962Z",
     "start_time": "2021-04-27T13:02:36.725522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, point [-99.99144364382136, -99.93861587635192], loss 686.3005008179159\n",
      "iter 50, point [-99.56362583488914, -96.92631128470325], loss 649.221346830939\n",
      "iter 100, point [-99.13580802595692, -94.02279509580971], loss 614.6970095624063\n",
      "iter 150, point [-98.7079902170247, -91.22404911807594], loss 582.543755023494\n",
      "iter 200, point [-98.28017240809248, -88.52620357520894], loss 552.5911329872217\n",
      "iter 250, point [-97.85235459916026, -85.9255316243737], loss 524.6810152322887\n",
      "iter 300, point [-97.42453679022805, -83.41844407682491], loss 498.6667034691001\n",
      "iter 350, point [-96.99671898129583, -81.00148431353688], loss 474.4121018974464\n",
      "iter 400, point [-96.56890117236361, -78.67132338862874], loss 451.7909497114133\n",
      "iter 450, point [-96.14108336343139, -76.42475531364933], loss 430.68610920670284\n",
      "iter 500, point [-95.71326555449917, -74.25869251604028], loss 410.988905460488\n",
      "iter 550, point [-95.28544774556696, -72.17016146534513], loss 392.5985138460824\n",
      "iter 600, point [-94.85762993663474, -70.15629846096763], loss 375.4213919156372\n",
      "iter 650, point [-94.42981212770252, -68.21434557551346], loss 359.3707524354014\n",
      "iter 700, point [-94.0019943187703, -66.34164674796719], loss 344.36607459115214\n",
      "iter 750, point [-93.57417650983808, -64.53564402117185], loss 330.33265059761464\n",
      "iter 800, point [-93.14635870090586, -62.793873918279786], loss 317.2011651461846\n",
      "iter 850, point [-92.71854089197365, -61.11396395304264], loss 304.907305311265\n",
      "iter 900, point [-92.29072308304143, -59.49362926899678], loss 293.3913987080144\n",
      "iter 950, point [-91.86290527410921, -57.930669402782904], loss 282.5980778542974\n",
      "iter 1000, point [-91.43508746517699, -56.4229651670156], loss 272.47596883802515\n",
      "iter 1050, point [-91.00726965624477, -54.968475648286564], loss 262.9774025287022\n",
      "iter 1100, point [-90.57945184731255, -53.56523531604897], loss 254.05814669965383\n",
      "iter 1150, point [-90.15163403838034, -52.21135123828792], loss 245.67715754581488\n",
      "iter 1200, point [-89.72381622944812, -50.90500040003218], loss 237.796349191773\n",
      "iter 1250, point [-89.2959984205159, -49.6444271209092], loss 230.3803798866218\n",
      "iter 1300, point [-88.86818061158368, -48.42794056808474], loss 223.3964536766492\n",
      "iter 1350, point [-88.44036280265146, -47.2539123610643], loss 216.81413643451378\n",
      "iter 1400, point [-88.01254499371925, -46.12077426496303], loss 210.60518520483126\n",
      "iter 1450, point [-87.58472718478703, -45.027015968976976], loss 204.74338990147896\n",
      "iter 1500, point [-87.15690937585481, -43.9711829469081], loss 199.20442646183588\n",
      "iter 1550, point [-86.72909156692259, -42.95187439671279], loss 193.96572062803054\n",
      "iter 1600, point [-86.30127375799037, -41.96774125615467], loss 189.00632158541163\n",
      "iter 1650, point [-85.87345594905815, -41.017484291751295], loss 184.3067847442463\n",
      "iter 1700, point [-85.44563814012594, -40.0998522583068], loss 179.84906300239203\n",
      "iter 1750, point [-85.01782033119372, -39.21364012642417], loss 175.61640587468244\n",
      "iter 1800, point [-84.5900025222615, -38.35768737548557], loss 171.59326591927967\n",
      "iter 1850, point [-84.16218471332928, -37.530876349682856], loss 167.76521193253296\n",
      "iter 1900, point [-83.73436690439706, -36.73213067476985], loss 164.11884842217898\n",
      "iter 1950, point [-83.30654909546485, -35.96041373329276], loss 160.64174090423475\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.w[5] = -100.\n",
    "        self.w[9] = -100.\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, graident_w5, gradient_w9, eta=0.01):\n",
    "        net.w[5] = net.w[5] - eta * gradient_w5\n",
    "        net.w[9] = net.w[9] - eta * gradient_w9\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        points = []\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            points.append([net.w[5][0], net.w[9][0]])\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            gradient_w5 = gradient_w[5][0]\n",
    "            gradient_w9 = gradient_w[9][0]\n",
    "            self.update(gradient_w5, gradient_w9, eta)\n",
    "            losses.append(L)\n",
    "            if i % 50 == 0:\n",
    "                print('iter {}, point {}, loss {}'.format(i, [net.w[5][0], net.w[9][0]], L))\n",
    "        return points, losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=2000\n",
    "# 启动训练\n",
    "points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对所有参数计算梯度并更新\n",
    "\n",
    "为了能给读者直观的感受，上面演示的梯度下降法的过程仅包含$w_5$和$w_9$两个参数。房价预测的完整模型，必须要对所有参数$w$和$b$进行求解。这需要将Network中的update和train函数进行修改。由于不在限定参与计算的参数（所有参数均参与计算），修改之后的代码反而更加简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.404325Z",
     "start_time": "2021-04-27T13:02:37.123935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9, loss 1.8984947314576224\n",
      "iter 19, loss 1.8031783384598725\n",
      "iter 29, loss 1.7135517565541092\n",
      "iter 39, loss 1.6292649416831264\n",
      "iter 49, loss 1.5499895293373231\n",
      "iter 59, loss 1.4754174896452612\n",
      "iter 69, loss 1.4052598659324693\n",
      "iter 79, loss 1.3392455915676864\n",
      "iter 89, loss 1.2771203802372917\n",
      "iter 99, loss 1.218645685090292\n",
      "iter 109, loss 1.1635977224791534\n",
      "iter 119, loss 1.111766556287068\n",
      "iter 129, loss 1.0629552390811503\n",
      "iter 139, loss 1.0169790065644477\n",
      "iter 149, loss 0.9736645220185994\n",
      "iter 159, loss 0.9328491676343147\n",
      "iter 169, loss 0.8943803798194309\n",
      "iter 179, loss 0.8581150257549611\n",
      "iter 189, loss 0.8239188186389669\n",
      "iter 199, loss 0.7916657692169988\n",
      "iter 209, loss 0.761237671346902\n",
      "iter 219, loss 0.7325236194855752\n",
      "iter 229, loss 0.7054195561163928\n",
      "iter 239, loss 0.6798278472589763\n",
      "iter 249, loss 0.6556568843183528\n",
      "iter 259, loss 0.6328207106387195\n",
      "iter 269, loss 0.6112386712285092\n",
      "iter 279, loss 0.59083508421862\n",
      "iter 289, loss 0.5715389327049418\n",
      "iter 299, loss 0.5532835757100347\n",
      "iter 309, loss 0.5360064770773406\n",
      "iter 319, loss 0.5196489511849665\n",
      "iter 329, loss 0.5041559244351538\n",
      "iter 339, loss 0.48947571154034963\n",
      "iter 349, loss 0.4755598056875569\n",
      "iter 359, loss 0.46236268171965056\n",
      "iter 369, loss 0.44984161152579916\n",
      "iter 379, loss 0.43795649088328303\n",
      "iter 389, loss 0.4266696770400226\n",
      "iter 399, loss 0.4159458363712466\n",
      "iter 409, loss 0.4057518014851036\n",
      "iter 419, loss 0.3960564371908221\n",
      "iter 429, loss 0.38683051477942226\n",
      "iter 439, loss 0.3780465941011246\n",
      "iter 449, loss 0.36967891295560856\n",
      "iter 459, loss 0.3617032833413179\n",
      "iter 469, loss 0.3540969941381647\n",
      "iter 479, loss 0.3468387198244131\n",
      "iter 489, loss 0.3399084348532937\n",
      "iter 499, loss 0.33328733333814486\n",
      "iter 509, loss 0.3269577537166779\n",
      "iter 519, loss 0.32090310808539985\n",
      "iter 529, loss 0.31510781591441284\n",
      "iter 539, loss 0.30955724187078903\n",
      "iter 549, loss 0.3042376374955925\n",
      "iter 559, loss 0.2991360864954391\n",
      "iter 569, loss 0.2942404534243286\n",
      "iter 579, loss 0.2895393355454012\n",
      "iter 589, loss 0.28502201767532415\n",
      "iter 599, loss 0.2806784298262616\n",
      "iter 609, loss 0.27649910747186535\n",
      "iter 619, loss 0.2724751542744919\n",
      "iter 629, loss 0.2685982071209627\n",
      "iter 639, loss 0.26486040332365085\n",
      "iter 649, loss 0.2612543498525749\n",
      "iter 659, loss 0.2577730944725093\n",
      "iter 669, loss 0.2544100986669443\n",
      "iter 679, loss 0.2511592122380609\n",
      "iter 689, loss 0.2480146494787638\n",
      "iter 699, loss 0.24497096681926708\n",
      "iter 709, loss 0.2420230418567802\n",
      "iter 719, loss 0.23916605368251415\n",
      "iter 729, loss 0.23639546442555454\n",
      "iter 739, loss 0.23370700193813698\n",
      "iter 749, loss 0.23109664355154746\n",
      "iter 759, loss 0.2285606008362593\n",
      "iter 769, loss 0.22609530530403904\n",
      "iter 779, loss 0.22369739499361888\n",
      "iter 789, loss 0.22136370188515422\n",
      "iter 799, loss 0.21909124009208833\n",
      "iter 809, loss 0.21687719478222933\n",
      "iter 819, loss 0.2147189117828403\n",
      "iter 829, loss 0.21261388782734392\n",
      "iter 839, loss 0.2105597614038757\n",
      "iter 849, loss 0.20855430416838638\n",
      "iter 859, loss 0.2065954128873093\n",
      "iter 869, loss 0.20468110187697833\n",
      "iter 879, loss 0.2028094959090178\n",
      "iter 889, loss 0.20097882355283644\n",
      "iter 899, loss 0.19918741092814596\n",
      "iter 909, loss 0.19743367584210875\n",
      "iter 919, loss 0.1957161222872899\n",
      "iter 929, loss 0.19403333527807182\n",
      "iter 939, loss 0.19238397600456975\n",
      "iter 949, loss 0.19076677728439412\n",
      "iter 959, loss 0.1891805392938162\n",
      "iter 969, loss 0.18762412556104593\n",
      "iter 979, loss 0.18609645920539716\n",
      "iter 989, loss 0.18459651940712488\n",
      "iter 999, loss 0.18312333809366155\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8VfWd//HXJ/tOEhLClrAJKC4IRBS1oq0itlZb61RptdgyD4bpvsx06mxO7fxmOu10napTRq2tv47WVmupWtG6FDeUoChE9jUhQMKWBMhKPvPHPdALJuQGbrg3976fj8d93Hu+53uTz8mB9zn33O85x9wdERFJHimxLkBERE4vBb+ISJJR8IuIJBkFv4hIklHwi4gkGQW/iEiSUfCLiCQZBb+ISJJR8IuIJJm0WBfQnZKSEh89enSsyxARGTCWL1++291LI+kbl8E/evRoqqqqYl2GiMiAYWZbI+2rQz0iIklGwS8ikmQU/CIiSabX4DezcjN7wcxWm1m1mX2pmz5mZj82sw1m9o6ZTQ2bN9fM1gePudFeABER6ZtIvtztBL7m7m+aWT6w3Myedfd3w/pcA4wPHhcC9wAXmlkxcAdQCXjw3kXuvi+qSyEiIhHrdY/f3Xe4+5vB62ZgNTDiuG7XA7/wkKVAoZkNA64GnnX3vUHYPwvMjuoSiIhIn/TpGL+ZjQamAK8fN2sEUBM2XRu09dQuIiIxEnHwm1ke8CjwZXdvOn52N2/xE7R39/Pnm1mVmVU1NDREWtZRrR2H+emfNvLy+t19fq+ISDKJKPjNLJ1Q6P/S3R/rpkstUB42PRKoO0H7e7j7QnevdPfK0tKITj47RkZqCguXbOLXy2t67ywiksQiGdVjwH3Aanf/fg/dFgGfCkb3XAQ0uvsOYDEwy8yKzKwImBW0RV1KivG+8SW8tH43XV26gbyISE8iGdVzCXArsNLMVgRtfw9UALj7fwNPAR8ENgCHgE8H8/aa2beAZcH77nT3vdEr/1iXTSjl8RV1VNc1ce7IQf31a0REBrReg9/dX6b7Y/XhfRz4XA/z7gfuP6nq+uh940OHiJasb1Dwi4j0IKHO3C3Nz2TSsAL+tK7vXw6LiCSLhAp+CB3ueXPrPppbO2JdiohIXErA4C+hs8t5beOeWJciIhKXEi74p40qIjs9lZc0nl9EpFsJF/yZaanMGDeYJet1nF9EpDsJF/wAl40vYeueQ2zdczDWpYiIxJ3EDP4JwbBOje4REXmPhAz+MSW5jCzK5k/rdJxfROR4CRn8ZsbMCaW8unE3bZ2HY12OiEhcScjgB3j/mUM41H6Y1zf12xUiREQGpIQN/ovHlZCZlsLza+pjXYqISFxJ2ODPzkjlkjNKeG7NLkKXEhIREUjg4IfQ4Z6avS1sbDgQ61JEROJGwgc/wHOrdbhHROSIhA7+4YXZnDWsgOd0nF9E5KiEDn6AD5w5hOVb97H/UHusSxERiQsJH/zvP2sIh7tc1+gXEQkkfPBPHlnI4NwMDesUEQkkfPCnphiXTxzCi2sb6DzcFetyRERirtfgN7P7zazezFb1MP9vzWxF8FhlZofNrDiYt8XMVgbzqqJdfKTef+YQGls6qNq6L1YliIjEjUj2+B8AZvc0092/6+7nu/v5wO3An9w9/DoJVwTzK0+t1JN32YQSMlJTeKZ6V6xKEBGJG70Gv7svASK94M0c4KFTqqgf5Gelc+n4EhZX79RZvCKS9KJ2jN/Mcgh9Mng0rNmBZ8xsuZnN7+X9882sysyqGhqiPwLn6rPL2L6/heq6pqj/bBGRgSSaX+5+GHjluMM8l7j7VOAa4HNmdllPb3b3he5e6e6VpaWlUSwr5MqzykgxWFy9M+o/W0RkIIlm8N/McYd53L0ueK4HfgtMj+Lv65PBeZlUji5W8ItI0otK8JvZIGAm8Luwtlwzyz/yGpgFdDsy6HSZffZQ1u06wObduheviCSvSIZzPgS8Bkw0s1ozm2dmC8xsQVi3jwLPuHt4opYBL5vZ28AbwJPu/nQ0i++rWWeXATrcIyLJLa23Du4+J4I+DxAa9hnetgmYfLKF9YeRRTmcM6KAxdU7WTBzXKzLERGJiYQ/c/d4V08aylvb9rOrqTXWpYiIxETSBf/sc4YC8IwO94hIkkq64D9jSB5jS3N5aqWCX0SSU9IFv5lx7bnDeH3zHuqbdbhHRJJP0gU/wLWTh9Pl8Aft9YtIEkrK4J9Qls+EsjyeeKcu1qWIiJx2SRn8ANeeN5xlW/axo7El1qWIiJxWSRz8wwB48p0dMa5EROT0StrgH1uax6RhBTyh4BeRJJO0wQ9w7eRhrKjZT83eQ7EuRUTktEnu4D93OABPrtRev4gkj6QO/orBOUweOUije0QkqSR18AN8ePJwVm1vYkP9gViXIiJyWiR98F83eTgpBr99qzbWpYiInBZJH/xDCrK4dHwpj79VR1eXbsQuIokv6YMf4IYpI9i+v4U3tuztvbOIyACn4Cd0Z66cjFR+++b2WJciItLvFPxATkYas88ZylMrd9DacTjW5YiI9KtI7rl7v5nVm1m3N0o3s8vNrNHMVgSPfw6bN9vM1prZBjP7RjQLj7Ybpoykua2TP67eFetSRET6VSR7/A8As3vp85K7nx887gQws1TgLuAaYBIwx8wmnUqx/WnGuMGUFWTqcI+IJLxeg9/dlwAn863ndGCDu29y93bgYeD6k/g5p0VqivGR80fwp3UN7DnQFutyRET6TbSO8c8ws7fN7A9mdnbQNgKoCetTG7TFrRumjqSzy3l8hc7kFZHEFY3gfxMY5e6Tgf8CHg/arZu+PQ6UN7P5ZlZlZlUNDQ1RKKvvJg7NZ/LIQTyyrAZ3jekXkcR0ysHv7k3ufiB4/RSQbmYlhPbwy8O6jgR63JV294XuXunulaWlpada1km76YIK1u5qZkXN/pjVICLSn045+M1sqJlZ8Hp68DP3AMuA8WY2xswygJuBRaf6+/rbhycPIzs9lUeqanrvLCIyAEUynPMh4DVgopnVmtk8M1tgZguCLjcCq8zsbeDHwM0e0gl8HlgMrAYecffq/lmM6MnPSudD5w1j0Yo6DrZ1xrocEZGoS+utg7vP6WX+T4Cf9DDvKeCpkystdm66oJzfLK/lyZU7+Hhlee9vEBEZQHTmbjcqRxUxtjSXR5bpcI+IJB4FfzfMjJsqy6nauk/X6ReRhKPg78ENU0eSlmL8atm2WJciIhJVCv4elOZnMuvsMn69vFYXbhORhKLgP4FbLhrF/kMdPPGObsYuIolDwX8CM8YO5owheTz42pZYlyIiEjUK/hMwM269aBRv1zbyts7kFZEEoeDvxQ1TR5CTkcqDS7fGuhQRkahQ8PciPyudj04Zwe/frmPfwfZYlyMicsoU/BG4dcYo2jq7+PVyndAlIgOfgj8CZw4tYProYv7/0m10delyzSIysCn4I3TrjFFs23uIF9bWx7oUEZFTouCP0OxzhjK0IIv7Xt4c61JERE6Jgj9C6akp3HbJaF7duIfqusZYlyMictIU/H0wZ3oFORmp3PuS9vpFZOBS8PfBoOx0Pl5Zzu/frmNnY2usyxEROSkK/j6ad+kYutx54NUtsS5FROSkKPj7qLw4h9nnDOV/X9+qWzOKyICk4D8J8y4dS1Nrp27ILiIDUiQ3W7/fzOrNbFUP8z9pZu8Ej1fNbHLYvC1mttLMVphZVTQLj6Vpo4qYWlHI/a9spvNwV6zLERHpk0j2+B8AZp9g/mZgprufB3wLWHjc/Cvc/Xx3rzy5EuPTX80cR83eFl2rX0QGnF6D392XAHtPMP9Vd98XTC4FRkaptrh21VllTCjL464XNugyDiIyoET7GP884A9h0w48Y2bLzWz+id5oZvPNrMrMqhoaGqJcVvSlpBifu+IM1tcf4Jl3d8a6HBGRiEUt+M3sCkLB/3dhzZe4+1TgGuBzZnZZT+9394XuXunulaWlpdEqq19de95wRg/O4ScvbMBde/0iMjBEJfjN7DzgXuB6d99zpN3d64LneuC3wPRo/L54kZpifPbyM1i1vYkX18X/pxQREYhC8JtZBfAYcKu7rwtrzzWz/COvgVlAtyODBrKPTBnB8EFZ3PW89vpFZGCIZDjnQ8BrwEQzqzWzeWa2wMwWBF3+GRgM3H3csM0y4GUzext4A3jS3Z/uh2WIqYy0FBZcPo6qrft4fXOP34GLiMQNi8e91MrKSq+qGjjD/ls7DvO+77zAuNJcHp4/I9bliEgSMrPlkQ6b15m7UZCVnspnLx/H0k17eWXD7liXIyJyQgr+KPnEhRUMH5TFdxev1bF+EYlrCv4oyUxL5QsfGM+Kmv08v0a3ZxSR+KXgj6Ibp41k1OAc/vOZdTqbV0TiloI/itJTU/jyleNZvaOJp1bpGj4iEp8U/FF23eQRjB+Sx/efXacrd4pIXFLwR1lqivG1WRPY1HCQR9+sjXU5IiLvoeDvB1efPZQpFYV875l1ukuXiMQdBX8/MDP+8UNnUd/cxsIlm2JdjojIMRT8/WTaqGI+dO4wFi7ZxK6m1liXIyJylIK/H3199kQ6u7r43jNrY12KiMhRCv5+NGpwLnNnjObXy2tZvaMp1uWIiAAK/n73hfePpyArnf/35GpdykFE4oKCv58Nyknny1eO5+UNu1lcvSvW5YiIKPhPh1svGsWZQ/P51hPv0tJ+ONbliEiSU/CfBmmpKXzzurPZvr+Fu17YEOtyRCTJKfhPkwvHDuYj5w9n4ZJNbN59MNbliEgSU/CfRn//wbPISEvhm7+v1he9IhIzEQW/md1vZvVm1u3N0i3kx2a2wczeMbOpYfPmmtn64DE3WoUPREMKsvjyleN5cW0Dz76rL3pFJDYi3eN/AJh9gvnXAOODx3zgHgAzKwbuAC4EpgN3mFnRyRabCOZePJqJZfncsaia5taOWJcjIkkoouB39yXA3hN0uR74hYcsBQrNbBhwNfCsu+91933As5x4A5Lw0lNT+PePncvOpla+u1hn9IrI6RetY/wjgJqw6dqgraf2pDa1oojbLh7Ng0u3UrXlRNtTEZHoi1bwWzdtfoL29/4As/lmVmVmVQ0NDVEqK379zayJDB+Uzd89+g5tnRrbLyKnT7SCvxYoD5seCdSdoP093H2hu1e6e2VpaWmUyopfuZlp/NsN57Kx4SB3Pa+x/SJy+kQr+BcBnwpG91wENLr7DmAxMMvMioIvdWcFbQLMnFDKDVNGcPeLG3URNxE5bSIdzvkQ8Bow0cxqzWyemS0wswVBl6eATcAG4H+AzwK4+17gW8Cy4HFn0CaBf7p2EoU5GXzlVyt0yEdETguLxxOJKisrvaqqKtZlnDbPr9nFZx6oYsHMcXzjmjNjXY6IDEBmttzdKyPpqzN348D7zyxjzvRyfrpkI8s0ykdE+pmCP07844cmUV6Uw1cfWcEB3aBdRPqRgj9O5Gam8b2PT6Z2Xwv/+sS7sS5HRBKYgj+OXDC6mAUzx/HwshqeeKfbUa8iIqdMwR9nvnrVBKZUFHL7oyvZtudQrMsRkQSk4I8z6akp/NecKZjB5x96k/bOrliXJCIJRsEfh0YW5fCdGyfzTm0j//H0mliXIyIJRsEfp2afM5TbLh7NfS9v1rX7RSSqFPxx7PYPnsm5Iwbx1V+tYFPDgViXIyIJQsEfxzLTUrnnlqmkp6Uw/8HlGt8vIlGh4I9zI4ty+MknprB590G+9sgKurri7xIbIjKwKPgHgIvHlXD7NWeyuHoXd7+oSziLyKlR8A8Q8y4dw/XnD+d7z67judX6sldETp6Cf4AwM759w3mcPbyALz70FtV1jbEuSUQGKAX/AJKdkcp9cy+gIDudeQ9UsbOxNdYlicgApOAfYMoKsrhv7gU0t3bwmQeWcVAjfUSkjxT8A9Ck4QXc9cmprN3VzBceeovDGukjIn2g4B+gLp84hH+57myeX1PPPz6+ini8k5qIxKe0SDqZ2WzgR0AqcK+7f/u4+T8Arggmc4Ah7l4YzDsMrAzmbXP366JRuMCtF41ix/4W7n5xI0U56Xx9tm7bKCK96zX4zSwVuAu4CqgFlpnZInc/ercQd/9KWP8vAFPCfkSLu58fvZIl3N9ePZH9LR3c/eJGCnPSmX/ZuFiXJCJxLpI9/unABnffBGBmDwPXAz3dJmoOcEd0ypPemBnfuv4cmlo6+Len1jAoO52bLqiIdVkiEsciOcY/AqgJm64N2t7DzEYBY4Dnw5qzzKzKzJaa2UdOulLpUWqK8f2Pn8/MCaXc/thK3b1LRE4okuC3btp6+ibxZuA37n44rK3C3SuBTwA/NLNuj0WY2fxgA1HV0NAQQVkSLiMthf++ZRqVo4r50sMrFP4i0qNIgr8WKA+bHgn0lCo3Aw+FN7h7XfC8CXiRY4//h/db6O6V7l5ZWloaQVlyvOyMVH726QuYVlGk8BeRHkUS/MuA8WY2xswyCIX7ouM7mdlEoAh4LaytyMwyg9clwCX0/N2AREFuZprCX0ROqNfgd/dO4PPAYmA18Ii7V5vZnWYWPjRzDvCwHzug/CygyszeBl4Avh0+Gkj6x/Hh//hb22NdkojEEYvHE38qKyu9qqoq1mUMeAfbOpn382Us3bSXb153NnMvHh3rkkSkn5jZ8uD71F7pzN0ElpuZxgOfns5Vk8q4Y1E1P/zjOp3hKyIK/kSXlZ7KPZ+cyo3TRvLDP67nXxZV6y5eIkkuoks2yMCWlprCdz52HoXZ6dz78mb2HGznP/9iMlnpqbEuTURiQMGfJFJSjH/40FmU5Gfy7T+soW5/C//zqUoG52XGujQROc10qCeJmBkLZo7j7k9OpbquiY/c/Qob6ptjXZaInGYK/iT0wXOH8au/mkFLexcfvftVXtmwO9YlichppOBPUueXF/L45y5m2KAsPnX/G9z70iaN+BFJEgr+JDayKIdH//pirjqrjH99cjVffHgFh9p1K0eRRKfgT3L5Wencc8tUvj57Ik++U8dH73qVLbsPxrosEelHCn7BzPjs5Wfw889MZ1dzKx/+ycs8tXJHrMsSkX6i4Jej3je+lN9//lLGlubx2V++ye2PraSl/XDvbxSRAUXBL8coL87hNwtmsGDmOB56Yxsf/snLrN7RFOuyRCSKFPzyHumpKXzjmjN5cN50Gls6uP6uV/jZK5t1qQeRBKHglx69b3wpf/jS+7hk3GC++ft3+cS9S9m251CsyxKRU6TglxMqycvk/tsu4D8+di7V25uY/aMl/OK1Ldr7FxnAFPzSKzPjpgsqWPyVy5g2qoh//l01n7z3dQ37FBmgFPwSseGF2fziM9P59g3nsmp7I7N+uIQf/XE9bZ0a+SMykCj4pU/MjJunV/DHr81k1qQyfvDHdcz+4Uu8vF7X+xEZKBT8clLKCrL4ySem8uC86bg7t9z3Ol946C3q9rfEujQR6UVEwW9ms81srZltMLNvdDP/NjNrMLMVweMvw+bNNbP1wWNuNIuX2Hvf+FKe/vJlfPnK8Syu3sn7v/ci339mLQfbdM0fkXjV683WzSwVWAdcBdQCy4A57v5uWJ/bgEp3//xx7y0GqoBKwIHlwDR333ei36mbrQ9MNXsP8d3Fa1n0dh2l+Zn8zawJ3DitnNQUi3VpIgkv2jdbnw5scPdN7t4OPAxcH2EtVwPPuvveIOyfBWZH+F4ZYMqLc/jxnCk89tmLKS/K5u8eXcmHfvwSL6yp1yWfReJIJME/AqgJm64N2o73MTN7x8x+Y2blfXwvZjbfzKrMrKqhoSGCsiReTa0o4tG/vpi7PjGVQ+2H+fQDy7jhnld5ef1ubQBE4kAkwd/d5/Tj//f+Hhjt7ucBfwR+3of3hhrdF7p7pbtXlpaWRlCWxDMz40PnDeO5r83k3284l12Nrdxy3+vctHApSzftiXV5IkktkuCvBcrDpkcCdeEd3H2Pu7cFk/8DTIv0vZLY0lNTmDO9ghf+9nLuvP5stuw+yM0Ll3LTT1/jxbU6BCQSC5EE/zJgvJmNMbMM4GZgUXgHMxsWNnkdsDp4vRiYZWZFZlYEzAraJMlkpqXyqRmjWfL1K/inayexdc8hbvvZMj7445f53YrtdB7uinWJIkmj1+B3907g84QCezXwiLtXm9mdZnZd0O2LZlZtZm8DXwRuC967F/gWoY3HMuDOoE2SVFZ6KvMuHcOSr1/Bd288j/bOw3zp4RVc8b0XefC1LRoGKnIa9DqcMxY0nDN5dHU5z67exT0vbmRFzX7ys9L4i2nl3DpjFGNKcmNdnsiA0ZfhnAp+iQvuzpvb9vPzV7fw1ModdHY5l08sZe6M0cycUEqKzgUQOSEFvwxo9U2t/O8b2/jl69toaG6jvDibv5hWzo3TRjK8MDvW5YnEJQW/JIT2zi6ert7Jw29s49WNezALXSLi45UjuWpSGZlpqbEuUSRuKPgl4dTsPcSvl9fym6oa6hpbKcxJ57rJw7n+/OFMrSjCTIeCJLkp+CVhHe5yXtmwm19V1fDsu7to7+xiRGE2104exnWThzNpWIE2ApKUFPySFJpbO3j23V38/u06Xlq/m84uZ2xpLh8+bzjXnDuUiWX52ghI0lDwS9LZe7Cdp1ftZNHb23l9817cobw4m1mThjJrUhnTRhWRlqrbT0jiUvBLUqtvbuW51fU8U72TVzbsof1wF0U56XzgrDKumlTGJWeUkJeZFusyRaJKwS8SONDWyZJ1DTxTvZPn19TT1NpJWooxdVQRMyeUMnNCKZOGFeg8ARnwFPwi3eg43EXVln0sWd/AknUNVNc1AVCSl8GlZ5Rw2YRSLh5XwtBBWTGuVKTvFPwiEWhobuOlYCPw0vrd7DnYDsDowTlcNHYwF44t5qKxgxk2SCeNSfxT8Iv0UVeX8+6OJpZu2sPSTXt5Y/MemlpDF4wbNTiHC8cUM33MYKZWFDKmJFejhSTuKPhFTtHhLmfNziaWbtrL0k17eGPzXhpbOgAoyklnSkURU8oLmTqqiMnlhfqyWGKuL8Gvf60i3UhNMc4ePoizhw9i3qVj6Opy1tcf4K1t+3hz2z7e3Laf59fUA2AGE8vymVJRyLkjCjlnRAETyvLJStclJSQ+aY9f5CQ1tnSwomY/b27dx1s1+1mxbd/Rw0NpKcb4snzOGV7AOSMGcc6IAs4aVkBOhva1pH/oUI9IDLg7tftaWLW9kVV1jaza3sSq7Y1HvzQ2g7EluZw5rICJZflMKMtn4tB8KopzSNVwUjlFOtQjEgNmRnlxDuXFOVxzbuhupO7Orqa2YzYGK2sbefKdHUffl5mWwviyvNCGoCyfCUNDG4VhBVk6v0D6hYJfpB+ZGUMHZTF0UBZXTio72n6wrZMN9QdYu6uZdTubWburmVc27OaxN7cf7ZOVnsLowbmMK81jTEkuY0tzg+c8BmWnx2JxJEFEFPxmNhv4EZAK3Ovu3z5u/leBvwQ6gQbgM+6+NZh3GFgZdN3m7tchkuRyM9OYXF7I5PLCY9obD3Wwrr6Zdbua2dRwkM27D1Jd18jT1Ts53PXnw7KDczOObghGl+RSUZxDeVEOFcU5FOaka7ipnFCvwW9mqcBdwFVALbDMzBa5+7th3d4CKt39kJn9NfAd4KZgXou7nx/lukUS0qCcdC4YXcwFo4uPaW/v7GLb3kNsajjA5t0Hj24Unl9Tz+4D7cf0zctMo7w4h4ri7NDGYHDo8FN5UQ4ji7I12kgi2uOfDmxw900AZvYwcD1wNPjd/YWw/kuBW6JZpEiyy0hL4YwheZwxJO898w60dVKz9xA1ew+xLXiu2dfCxoaDvLi2gbbOrmP6l+ZnMnxQFsMGZTOsMIsRhdlHXw8flE1pfqa+bE5wkQT/CKAmbLoWuPAE/ecBfwibzjKzKkKHgb7t7o/3uUoR6VFeZhpnDQsNFz2eu9PQ3EbNvtBGYdueFur2t1DX2MKGhgMsWd/AofbDx7wnLcUoK8hieOGfNw5DC7IoK8hiSH4mQ/KzGFKQqU8OA1gkwd/dpr/bMaBmdgtQCcwMa65w9zozGws8b2Yr3X1jN++dD8wHqKioiKAsEemNmTGkIIshBVlMG1X8nvnuTlNLJ3WNLexobKFufyt1+1vY0Rh6XlGzn6dXtdJ+uOs97y3ISgv97PzMoxuF0vxMhhRkURY8D8nPJFdnNcedSNZILVAeNj0SqDu+k5ldCfwDMNPd2460u3td8LzJzF4EpgDvCX53XwgshNA4/sgXQUROlpkxKCedQTnp3X5igNB1jPYdaqe+uY1dTa3UN7fR0NxGfVMru5raqG9uZdmWvdQ3t9He+d4NRE5GKoPzMijOzaQkN4PBeRkMzstk8JHXuZkU52ZQkhd6zkjTDXP6WyTBvwwYb2ZjgO3AzcAnwjuY2RTgp8Bsd68Pay8CDrl7m5mVAJcQ+uJXRAaIlBQLBXVeZo8bBwh9emhs6aC+uY36YIOwq6mNPQfa2HOwnd0H2tjZ1Ep1XRN7DrbRcbj7/buCrLRjNgzFuZkU56ZTmJ1BYU46RTkZFOWmMyg7g6KcdAZlp+vuan3Ua/C7e6eZfR5YTGg45/3uXm1mdwJV7r4I+C6QB/w6GEZ2ZNjmWcBPzawLSCF0jP/dbn+RiAxoZkZhTgaFORlMKMs/YV93p6m1kz0H2th7sJ3dB9rZc7CNPQfag+nQ6827D1K1ZR/7WzqOGc56vPystNAGISc9qCG0gSjMSacwO52i3FBdRzYUBVnp5GelJe0GQ5dsEJG45+40t3Wy/2AH+w61s+9QO40tHew72M6+Qx3sP9TO/paOo6/3HWpn/6EOmoNrJ/UkNyOVguw/bwwKstOC53QKstKC52Pbj/TNy0qLq9FPumSDiCQUMwsFb1Y6FYNzIn5fx+EuGluCDcOh0IahsaWDppYOmlo7aGrpDJ5D03X7W1nT2kxTSwfNbZ30tl+cnxnaOORnpR39FJGXlUZeZug5P/PI63TyMtPIzwo9/jw/naz0lNN+wp2CX0QSVnpqCiV5mZTkZfb5vV1dzoH2ztBG4ZgNRGePG49dza1sbOjkQFsnTa2d3X7ZfbzUFAttCDLTGFGYzSMLZpzMovaJgl9EpBspKX/+lEHRyf2Mts4HthhZAAAEvklEQVTDHGw7zIHWTprbOjjQGtooNLd20tzWGUx3BPM7yTxNI5oU/CIi/SQzLZXMtFSKczNiXcoxkvMrbRGRJKbgFxFJMgp+EZEko+AXEUkyCn4RkSSj4BcRSTIKfhGRJKPgFxFJMnF5kTYzawC2nuTbS4DdUSxnINAyJwctc+I7leUd5e6lkXSMy+A/FWZWFekV6hKFljk5aJkT3+laXh3qERFJMgp+EZEkk4jBvzDWBcSAljk5aJkT32lZ3oQ7xi8iIieWiHv8IiJyAgkT/GY228zWmtkGM/tGrOuJFjMrN7MXzGy1mVWb2ZeC9mIze9bM1gfPRUG7mdmPg7/DO2Y2NbZLcPLMLNXM3jKzJ4LpMWb2erDMvzKzjKA9M5jeEMwfHcu6T5aZFZrZb8xsTbC+ZyT6ejazrwT/rleZ2UNmlpVo69nM7jezejNbFdbW5/VqZnOD/uvNbO6p1JQQwW9mqcBdwDXAJGCOmU2KbVVR0wl8zd3PAi4CPhcs2zeA59x9PPBcMA2hv8H44DEfuOf0lxw1XwJWh03/B/CDYJn3AfOC9nnAPnc/A/hB0G8g+hHwtLufCUwmtOwJu57NbATwRaDS3c8BUoGbSbz1/AAw+7i2Pq1XMysG7gAuBKYDdxzZWJwUdx/wD2AGsDhs+nbg9ljX1U/L+jvgKmAtMCxoGwasDV7/FJgT1v9ov4H0AEYG/yHeDzwBGKETW9KOX+fAYmBG8Dot6GexXoY+Lm8BsPn4uhN5PQMjgBqgOFhvTwBXJ+J6BkYDq052vQJzgJ+GtR/Tr6+PhNjj58//gI6oDdoSSvDRdgrwOlDm7jsAguchQbdE+Vv8EPg6cORu1YOB/e7eGUyHL9fRZQ7mNwb9B5KxQAPws+Dw1r1mlksCr2d33w78J7AN2EFovS0nsdfzEX1dr1Fd34kS/NZNW0INVzKzPOBR4Mvu3nSirt20Dai/hZldC9S7+/Lw5m66egTzBoo0YCpwj7tPAQ7y54//3RnwyxwcqrgeGAMMB3IJHeo4XiKt5970tIxRXfZECf5aoDxseiRQF6Naos7M0gmF/i/d/bGgeZeZDQvmDwPqg/ZE+FtcAlxnZluAhwkd7vkhUGhmaUGf8OU6uszB/EHA3tNZcBTUArXu/now/RtCG4JEXs9XApvdvcHdO4DHgItJ7PV8RF/Xa1TXd6IE/zJgfDAaIIPQF0SLYlxTVJiZAfcBq939+2GzFgFHvtmfS+jY/5H2TwWjAy4CGo98pBwo3P12dx/p7qMJrcvn3f2TwAvAjUG345f5yN/ixqD/gNoTdPedQI2ZTQyaPgC8SwKvZ0KHeC4ys5zg3/mRZU7Y9Rymr+t1MTDLzIqCT0qzgraTE+svPaL45ckHgXXARuAfYl1PFJfrUkIf6d4BVgSPDxI6tvkcsD54Lg76G6ERThuBlYRGTMR8OU5h+S8HnghejwXeADYAvwYyg/asYHpDMH9srOs+yWU9H6gK1vXjQFGir2fgm8AaYBXwIJCZaOsZeIjQdxgdhPbc553MegU+Eyz7BuDTp1KTztwVEUkyiXKoR0REIqTgFxFJMgp+EZEko+AXEUkyCn4RkSSj4BcRSTIKfhGRJKPgFxFJMv8HyjprbjIzUEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            self.update(gradient_w, gradient_b, eta)\n",
    "            losses.append(L)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print('iter {}, loss {}'.format(i, L))\n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=1000\n",
    "# 启动训练\n",
    "losses = net.train(x,y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小批量随机梯度下降法（Mini-batch Stochastic Gradient Descent）\n",
    "\n",
    "\n",
    "在上述程序中，每次迭代的时候均基于数据集中的全部数据进行计算。但在实际问题中数据集往往非常大，如果每次计算都使用全部的数据来计算损失函数和梯度，效率非常低。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失，然后更新参数。这种方法被称作小批量随机梯度下降法（Mini-batch Stochastic Gradient Descent），简称SGD。每次迭代时抽取出来的一批数据被称为一个min-batch，一个mini-batch所包含的样本数目称为batch_size。当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮的训练，也叫一个epoch。启动训练时，可以将训练的轮数num_epochs和batch_size作为参数传入。\n",
    "\n",
    "下面结合程序介绍具体的实现过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.430543Z",
     "start_time": "2021-04-27T13:02:37.405352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data中一共包含404条数据，如果batch_size=10，即取前0-9号样本作为第一个mini-batch，命名train_data1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.445271Z",
     "start_time": "2021-04-27T13:02:37.432569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1 = train_data[0:10]\n",
    "train_data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用train_data1的数据（0-9号样本）计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.461215Z",
     "start_time": "2021-04-27T13:02:37.447269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9001866101467375]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(13)\n",
    "x = train_data1[:, :-1]\n",
    "y = train_data1[:, -1:]\n",
    "loss = net.train(x, y, iterations=1, eta=0.01)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再取出10-19号样本作为第二个mini-batch，计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.475113Z",
     "start_time": "2021-04-27T13:02:37.462215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8903272433979659]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2 = train_data[10:19]\n",
    "x = train_data1[:, :-1]\n",
    "y = train_data1[:, -1:]\n",
    "loss = net.train(x, y, iterations=1, eta=0.01)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按此方法不断的取出新的mini-batch并逐渐更新网络参数。\n",
    "下面的程序可以将train_data分成大小为batch_size的多个mini_batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.490638Z",
     "start_time": "2021-04-27T13:02:37.477427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of mini_batches is  41\n",
      "first mini_batch shape  (10, 14)\n",
      "last mini_batch shape  (4, 14)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n = len(train_data)\n",
    "mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "print('total number of mini_batches is ', len(mini_batches))\n",
    "print('first mini_batch shape ', mini_batches[0].shape)\n",
    "print('last mini_batch shape ', mini_batches[-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码将train_data分成 $\\frac{404}{10} + 1 = 41$ 个 mini_batch了，其中前40个mini_batch，每个均含有10个样本，最后一个mini_batch只含有4个样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，我们这里是按顺序取出mini_batch的，而SGD里面是随机的抽取一部分样本代表总体。为了实现随机抽样的效果，我们先将train_data里面的样本顺序随机打乱，然后再抽取mini_batch。随机打乱样本顺序，需要用到np.random.shuffle函数，下面先介绍它的用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.503895Z",
     "start_time": "2021-04-27T13:02:37.492632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "after shuffle [ 7  2 11  3  8  6 12  1  4  5 10  9]\n"
     ]
    }
   ],
   "source": [
    "# 新建一个array\n",
    "a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "print('before shuffle', a)\n",
    "np.random.shuffle(a)\n",
    "print('after shuffle', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次运行上面的代码，可以发现每次执行shuffle函数后的数字顺序均不同。\n",
    "上面举的是一个1维数组乱序的案例，我们在观察下2维数组乱序后的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.519452Z",
     "start_time": "2021-04-27T13:02:37.505465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "after shuffle\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 9 10]\n",
      " [11 12]\n",
      " [ 7  8]]\n"
     ]
    }
   ],
   "source": [
    "# 新建一个array\n",
    "a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "a = a.reshape([6, 2])\n",
    "print('before shuffle\\n', a)\n",
    "np.random.shuffle(a)\n",
    "print('after shuffle\\n', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察运行结果可发现，数组的元素在第0维被随机打乱，但第1维的顺序保持不变。例如数字2仍然紧挨在数字1的后面，数字8仍然紧挨在数字7的后面，而第二维的[3, 4]并不排在[1, 2]的后面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上随机乱序和抽取mini_batch的步骤，我们可以改写训练过程如下。每个随机抽取的mini-batch数据，输入到模型中用于参数训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.550775Z",
     "start_time": "2021-04-27T13:02:37.521609Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 打乱样本顺序\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# 将train_data分成多个mini_batch\n",
    "batch_size = 10\n",
    "n = len(train_data)\n",
    "mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "\n",
    "# 依次使用每个mini_batch的数据\n",
    "for mini_batch in mini_batches:\n",
    "    x = mini_batch[:, :-1]\n",
    "    y = mini_batch[:, -1:]\n",
    "    loss = net.train(x, y, iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这部分实现SGD算法的代码集成到Network类中的train函数中，最终的完整代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:02:37.814914Z",
     "start_time": "2021-04-27T13:02:37.553339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 0.6273\n",
      "Epoch   0 / iter   1, loss = 0.4835\n",
      "Epoch   0 / iter   2, loss = 0.5830\n",
      "Epoch   0 / iter   3, loss = 0.5466\n",
      "Epoch   0 / iter   4, loss = 0.2147\n",
      "Epoch   1 / iter   0, loss = 0.6645\n",
      "Epoch   1 / iter   1, loss = 0.4875\n",
      "Epoch   1 / iter   2, loss = 0.4707\n",
      "Epoch   1 / iter   3, loss = 0.4153\n",
      "Epoch   1 / iter   4, loss = 0.1402\n",
      "Epoch   2 / iter   0, loss = 0.5897\n",
      "Epoch   2 / iter   1, loss = 0.4373\n",
      "Epoch   2 / iter   2, loss = 0.4631\n",
      "Epoch   2 / iter   3, loss = 0.3960\n",
      "Epoch   2 / iter   4, loss = 0.2340\n",
      "Epoch   3 / iter   0, loss = 0.4139\n",
      "Epoch   3 / iter   1, loss = 0.5635\n",
      "Epoch   3 / iter   2, loss = 0.3807\n",
      "Epoch   3 / iter   3, loss = 0.3975\n",
      "Epoch   3 / iter   4, loss = 0.1207\n",
      "Epoch   4 / iter   0, loss = 0.3786\n",
      "Epoch   4 / iter   1, loss = 0.4474\n",
      "Epoch   4 / iter   2, loss = 0.4019\n",
      "Epoch   4 / iter   3, loss = 0.4352\n",
      "Epoch   4 / iter   4, loss = 0.0435\n",
      "Epoch   5 / iter   0, loss = 0.4387\n",
      "Epoch   5 / iter   1, loss = 0.3886\n",
      "Epoch   5 / iter   2, loss = 0.3182\n",
      "Epoch   5 / iter   3, loss = 0.4189\n",
      "Epoch   5 / iter   4, loss = 0.1741\n",
      "Epoch   6 / iter   0, loss = 0.3191\n",
      "Epoch   6 / iter   1, loss = 0.3601\n",
      "Epoch   6 / iter   2, loss = 0.4199\n",
      "Epoch   6 / iter   3, loss = 0.3289\n",
      "Epoch   6 / iter   4, loss = 1.2691\n",
      "Epoch   7 / iter   0, loss = 0.3202\n",
      "Epoch   7 / iter   1, loss = 0.2855\n",
      "Epoch   7 / iter   2, loss = 0.4129\n",
      "Epoch   7 / iter   3, loss = 0.3331\n",
      "Epoch   7 / iter   4, loss = 0.2218\n",
      "Epoch   8 / iter   0, loss = 0.2368\n",
      "Epoch   8 / iter   1, loss = 0.3457\n",
      "Epoch   8 / iter   2, loss = 0.3339\n",
      "Epoch   8 / iter   3, loss = 0.3812\n",
      "Epoch   8 / iter   4, loss = 0.0534\n",
      "Epoch   9 / iter   0, loss = 0.3567\n",
      "Epoch   9 / iter   1, loss = 0.4033\n",
      "Epoch   9 / iter   2, loss = 0.1926\n",
      "Epoch   9 / iter   3, loss = 0.2803\n",
      "Epoch   9 / iter   4, loss = 0.1557\n",
      "Epoch  10 / iter   0, loss = 0.3435\n",
      "Epoch  10 / iter   1, loss = 0.2790\n",
      "Epoch  10 / iter   2, loss = 0.3456\n",
      "Epoch  10 / iter   3, loss = 0.2076\n",
      "Epoch  10 / iter   4, loss = 0.0935\n",
      "Epoch  11 / iter   0, loss = 0.3024\n",
      "Epoch  11 / iter   1, loss = 0.2517\n",
      "Epoch  11 / iter   2, loss = 0.2797\n",
      "Epoch  11 / iter   3, loss = 0.2989\n",
      "Epoch  11 / iter   4, loss = 0.0301\n",
      "Epoch  12 / iter   0, loss = 0.2507\n",
      "Epoch  12 / iter   1, loss = 0.2563\n",
      "Epoch  12 / iter   2, loss = 0.2971\n",
      "Epoch  12 / iter   3, loss = 0.2833\n",
      "Epoch  12 / iter   4, loss = 0.0597\n",
      "Epoch  13 / iter   0, loss = 0.2827\n",
      "Epoch  13 / iter   1, loss = 0.2094\n",
      "Epoch  13 / iter   2, loss = 0.2417\n",
      "Epoch  13 / iter   3, loss = 0.2985\n",
      "Epoch  13 / iter   4, loss = 0.4036\n",
      "Epoch  14 / iter   0, loss = 0.3085\n",
      "Epoch  14 / iter   1, loss = 0.2015\n",
      "Epoch  14 / iter   2, loss = 0.1830\n",
      "Epoch  14 / iter   3, loss = 0.2978\n",
      "Epoch  14 / iter   4, loss = 0.0630\n",
      "Epoch  15 / iter   0, loss = 0.2342\n",
      "Epoch  15 / iter   1, loss = 0.2780\n",
      "Epoch  15 / iter   2, loss = 0.2571\n",
      "Epoch  15 / iter   3, loss = 0.1838\n",
      "Epoch  15 / iter   4, loss = 0.0627\n",
      "Epoch  16 / iter   0, loss = 0.1896\n",
      "Epoch  16 / iter   1, loss = 0.1966\n",
      "Epoch  16 / iter   2, loss = 0.2018\n",
      "Epoch  16 / iter   3, loss = 0.3257\n",
      "Epoch  16 / iter   4, loss = 0.1268\n",
      "Epoch  17 / iter   0, loss = 0.1990\n",
      "Epoch  17 / iter   1, loss = 0.2031\n",
      "Epoch  17 / iter   2, loss = 0.2662\n",
      "Epoch  17 / iter   3, loss = 0.2128\n",
      "Epoch  17 / iter   4, loss = 0.0133\n",
      "Epoch  18 / iter   0, loss = 0.1780\n",
      "Epoch  18 / iter   1, loss = 0.1575\n",
      "Epoch  18 / iter   2, loss = 0.2547\n",
      "Epoch  18 / iter   3, loss = 0.2544\n",
      "Epoch  18 / iter   4, loss = 0.2007\n",
      "Epoch  19 / iter   0, loss = 0.1657\n",
      "Epoch  19 / iter   1, loss = 0.2000\n",
      "Epoch  19 / iter   2, loss = 0.2045\n",
      "Epoch  19 / iter   3, loss = 0.2524\n",
      "Epoch  19 / iter   4, loss = 0.0632\n",
      "Epoch  20 / iter   0, loss = 0.1629\n",
      "Epoch  20 / iter   1, loss = 0.1895\n",
      "Epoch  20 / iter   2, loss = 0.2523\n",
      "Epoch  20 / iter   3, loss = 0.1896\n",
      "Epoch  20 / iter   4, loss = 0.0918\n",
      "Epoch  21 / iter   0, loss = 0.1583\n",
      "Epoch  21 / iter   1, loss = 0.2322\n",
      "Epoch  21 / iter   2, loss = 0.1567\n",
      "Epoch  21 / iter   3, loss = 0.2089\n",
      "Epoch  21 / iter   4, loss = 0.2035\n",
      "Epoch  22 / iter   0, loss = 0.2273\n",
      "Epoch  22 / iter   1, loss = 0.1427\n",
      "Epoch  22 / iter   2, loss = 0.1712\n",
      "Epoch  22 / iter   3, loss = 0.1826\n",
      "Epoch  22 / iter   4, loss = 0.2878\n",
      "Epoch  23 / iter   0, loss = 0.1685\n",
      "Epoch  23 / iter   1, loss = 0.1622\n",
      "Epoch  23 / iter   2, loss = 0.1499\n",
      "Epoch  23 / iter   3, loss = 0.2329\n",
      "Epoch  23 / iter   4, loss = 0.1486\n",
      "Epoch  24 / iter   0, loss = 0.1617\n",
      "Epoch  24 / iter   1, loss = 0.2083\n",
      "Epoch  24 / iter   2, loss = 0.1442\n",
      "Epoch  24 / iter   3, loss = 0.1740\n",
      "Epoch  24 / iter   4, loss = 0.1641\n",
      "Epoch  25 / iter   0, loss = 0.1159\n",
      "Epoch  25 / iter   1, loss = 0.2064\n",
      "Epoch  25 / iter   2, loss = 0.1690\n",
      "Epoch  25 / iter   3, loss = 0.1778\n",
      "Epoch  25 / iter   4, loss = 0.0159\n",
      "Epoch  26 / iter   0, loss = 0.1730\n",
      "Epoch  26 / iter   1, loss = 0.1861\n",
      "Epoch  26 / iter   2, loss = 0.1387\n",
      "Epoch  26 / iter   3, loss = 0.1486\n",
      "Epoch  26 / iter   4, loss = 0.1090\n",
      "Epoch  27 / iter   0, loss = 0.1393\n",
      "Epoch  27 / iter   1, loss = 0.1775\n",
      "Epoch  27 / iter   2, loss = 0.1564\n",
      "Epoch  27 / iter   3, loss = 0.1245\n",
      "Epoch  27 / iter   4, loss = 0.7611\n",
      "Epoch  28 / iter   0, loss = 0.1470\n",
      "Epoch  28 / iter   1, loss = 0.1211\n",
      "Epoch  28 / iter   2, loss = 0.1285\n",
      "Epoch  28 / iter   3, loss = 0.1854\n",
      "Epoch  28 / iter   4, loss = 0.5240\n",
      "Epoch  29 / iter   0, loss = 0.1740\n",
      "Epoch  29 / iter   1, loss = 0.0898\n",
      "Epoch  29 / iter   2, loss = 0.1392\n",
      "Epoch  29 / iter   3, loss = 0.1842\n",
      "Epoch  29 / iter   4, loss = 0.0251\n",
      "Epoch  30 / iter   0, loss = 0.0978\n",
      "Epoch  30 / iter   1, loss = 0.1529\n",
      "Epoch  30 / iter   2, loss = 0.1640\n",
      "Epoch  30 / iter   3, loss = 0.1503\n",
      "Epoch  30 / iter   4, loss = 0.0975\n",
      "Epoch  31 / iter   0, loss = 0.1399\n",
      "Epoch  31 / iter   1, loss = 0.1595\n",
      "Epoch  31 / iter   2, loss = 0.1209\n",
      "Epoch  31 / iter   3, loss = 0.1203\n",
      "Epoch  31 / iter   4, loss = 0.2008\n",
      "Epoch  32 / iter   0, loss = 0.1501\n",
      "Epoch  32 / iter   1, loss = 0.1310\n",
      "Epoch  32 / iter   2, loss = 0.1065\n",
      "Epoch  32 / iter   3, loss = 0.1489\n",
      "Epoch  32 / iter   4, loss = 0.0818\n",
      "Epoch  33 / iter   0, loss = 0.1401\n",
      "Epoch  33 / iter   1, loss = 0.1367\n",
      "Epoch  33 / iter   2, loss = 0.0970\n",
      "Epoch  33 / iter   3, loss = 0.1481\n",
      "Epoch  33 / iter   4, loss = 0.0711\n",
      "Epoch  34 / iter   0, loss = 0.1157\n",
      "Epoch  34 / iter   1, loss = 0.1050\n",
      "Epoch  34 / iter   2, loss = 0.1378\n",
      "Epoch  34 / iter   3, loss = 0.1505\n",
      "Epoch  34 / iter   4, loss = 0.0429\n",
      "Epoch  35 / iter   0, loss = 0.1096\n",
      "Epoch  35 / iter   1, loss = 0.1279\n",
      "Epoch  35 / iter   2, loss = 0.1715\n",
      "Epoch  35 / iter   3, loss = 0.0888\n",
      "Epoch  35 / iter   4, loss = 0.0473\n",
      "Epoch  36 / iter   0, loss = 0.1350\n",
      "Epoch  36 / iter   1, loss = 0.0781\n",
      "Epoch  36 / iter   2, loss = 0.1458\n",
      "Epoch  36 / iter   3, loss = 0.1288\n",
      "Epoch  36 / iter   4, loss = 0.0421\n",
      "Epoch  37 / iter   0, loss = 0.1083\n",
      "Epoch  37 / iter   1, loss = 0.0972\n",
      "Epoch  37 / iter   2, loss = 0.1513\n",
      "Epoch  37 / iter   3, loss = 0.1236\n",
      "Epoch  37 / iter   4, loss = 0.0366\n",
      "Epoch  38 / iter   0, loss = 0.1204\n",
      "Epoch  38 / iter   1, loss = 0.1341\n",
      "Epoch  38 / iter   2, loss = 0.1109\n",
      "Epoch  38 / iter   3, loss = 0.0905\n",
      "Epoch  38 / iter   4, loss = 0.3906\n",
      "Epoch  39 / iter   0, loss = 0.0923\n",
      "Epoch  39 / iter   1, loss = 0.1094\n",
      "Epoch  39 / iter   2, loss = 0.1295\n",
      "Epoch  39 / iter   3, loss = 0.1239\n",
      "Epoch  39 / iter   4, loss = 0.0684\n",
      "Epoch  40 / iter   0, loss = 0.1188\n",
      "Epoch  40 / iter   1, loss = 0.0984\n",
      "Epoch  40 / iter   2, loss = 0.1067\n",
      "Epoch  40 / iter   3, loss = 0.1057\n",
      "Epoch  40 / iter   4, loss = 0.4602\n",
      "Epoch  41 / iter   0, loss = 0.1478\n",
      "Epoch  41 / iter   1, loss = 0.0980\n",
      "Epoch  41 / iter   2, loss = 0.0921\n",
      "Epoch  41 / iter   3, loss = 0.1020\n",
      "Epoch  41 / iter   4, loss = 0.0430\n",
      "Epoch  42 / iter   0, loss = 0.0991\n",
      "Epoch  42 / iter   1, loss = 0.0994\n",
      "Epoch  42 / iter   2, loss = 0.1270\n",
      "Epoch  42 / iter   3, loss = 0.0988\n",
      "Epoch  42 / iter   4, loss = 0.1176\n",
      "Epoch  43 / iter   0, loss = 0.1286\n",
      "Epoch  43 / iter   1, loss = 0.1013\n",
      "Epoch  43 / iter   2, loss = 0.1066\n",
      "Epoch  43 / iter   3, loss = 0.0779\n",
      "Epoch  43 / iter   4, loss = 0.1481\n",
      "Epoch  44 / iter   0, loss = 0.0840\n",
      "Epoch  44 / iter   1, loss = 0.0858\n",
      "Epoch  44 / iter   2, loss = 0.1388\n",
      "Epoch  44 / iter   3, loss = 0.1000\n",
      "Epoch  44 / iter   4, loss = 0.0313\n",
      "Epoch  45 / iter   0, loss = 0.0896\n",
      "Epoch  45 / iter   1, loss = 0.1173\n",
      "Epoch  45 / iter   2, loss = 0.0916\n",
      "Epoch  45 / iter   3, loss = 0.1043\n",
      "Epoch  45 / iter   4, loss = 0.0074\n",
      "Epoch  46 / iter   0, loss = 0.1008\n",
      "Epoch  46 / iter   1, loss = 0.0915\n",
      "Epoch  46 / iter   2, loss = 0.0877\n",
      "Epoch  46 / iter   3, loss = 0.1139\n",
      "Epoch  46 / iter   4, loss = 0.0292\n",
      "Epoch  47 / iter   0, loss = 0.0679\n",
      "Epoch  47 / iter   1, loss = 0.0987\n",
      "Epoch  47 / iter   2, loss = 0.0929\n",
      "Epoch  47 / iter   3, loss = 0.1098\n",
      "Epoch  47 / iter   4, loss = 0.4838\n",
      "Epoch  48 / iter   0, loss = 0.0693\n",
      "Epoch  48 / iter   1, loss = 0.1095\n",
      "Epoch  48 / iter   2, loss = 0.1128\n",
      "Epoch  48 / iter   3, loss = 0.0890\n",
      "Epoch  48 / iter   4, loss = 0.1008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  49 / iter   0, loss = 0.0724\n",
      "Epoch  49 / iter   1, loss = 0.0804\n",
      "Epoch  49 / iter   2, loss = 0.0919\n",
      "Epoch  49 / iter   3, loss = 0.1233\n",
      "Epoch  49 / iter   4, loss = 0.1849\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        #np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        N = x.shape[0]\n",
    "        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = 1. / N * np.sum(z-y)\n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "            \n",
    "                \n",
    "    def train(self, training_data, num_epoches, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epoches):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机的打乱，\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w, gradient_b = self.gradient(x, y)\n",
    "                self.update(gradient_w, gradient_b, eta)\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                 format(epoch_id, iter_id, loss))\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epoches=50, batch_size=100, eta=0.1)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "我们详细讲解了如何使用numpy实现梯度下降算法，构建并训练了一个简单的线性模型实现波士顿房价预测，可以总结出，使用神经网络建模房价预测有三个要点：\n",
    "\n",
    "- 构建网络，初始化参数w和b，定义预测和损失函数的计算方法。\n",
    "\n",
    "- 随机选择初始点，建立梯度的计算方法，和参数更新方式。\n",
    "\n",
    "- 从总的数据集中抽取部分数据作为一个mini_batch，计算梯度并更新参数，不断迭代直到损失函数几乎不再下降。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
